{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1KKr0f9G6lS"
      },
      "outputs": [],
      "source": [
        "                                                                                                   #Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                                                       #Assignment"
      ],
      "metadata": {
        "id": "PCNzUnRRL5L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1 What is a parameter?\n",
        "\"\"\"Ans\n",
        "A parameter can be defined as the number of arguments which any functions take in order to get desired results.\n",
        " Some parameters in function maybe a required parameter, it means that the function will not perform itself without those parameters.\"\"\""
      ],
      "metadata": {
        "id": "34r0xCRpL-Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2 What is correlation? What does negative correlation mean?\n",
        "\"\"\"Ans\n",
        "Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It ranges from -1 to +1:\n",
        " +1 means a perfect positive correlation (as one increases, the other increases).\n",
        " 0 means no correlation (no relationship).\n",
        " -1 means a perfect negative correlation (as one increases, the other decreases).\n",
        " eg:- A negative correlation occurs when one variable increases, the other decreases. Let's say your game time increases as your study time decreases.\"\"\""
      ],
      "metadata": {
        "id": "_7At42m6MPPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3 Define Machine Learning. What are the main components in Machine Learning?\n",
        "\"\"\"Ans\n",
        "Machine learning is a branch of AI which deals with making machines which can learn from patterns,trends and past experiences to perform tasks without any human intervention.\n",
        "It's main componenet are:-\n",
        "Data : Raw Information\n",
        "Features: Specific patterns to make predictions\n",
        "Model: The algorithm that learns patterns from data.\n",
        "Training: Feeding data in model to learn\n",
        "Evaluation: Measuring how well the model performs using metrics\n",
        "Inference: Using the trained model to make decisions or predictions on unseen data\n",
        "Hyperparameters: Settings that control the learning process\n",
        "Loss Function:How far off the model’s predictions are from the actual values.\n",
        "Optimization Algorithm:Used to minimize the loss function and improve the model.\"\"\""
      ],
      "metadata": {
        "id": "WtElasdtM3Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4 How does loss value help in determining whether the model is good or not?\n",
        "\"\"\"Ans\n",
        "Loss value is the output of loss function. Here is how we interpret it, if the loss value generated is high, it means that there is large difference between actual output and calculated.\n",
        " It means the ML predictions are incorrect and the model is not working properly. We need to ensure that the loss value be as low as possible.\"\"\"\n"
      ],
      "metadata": {
        "id": "5gEG_gjfNxXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5 What are continuous and categorical variables?\n",
        "\"\"\"Ans\n",
        "Continuous variables and categorical variables are two types of data:\n",
        "\n",
        "1. Continuous Variables\n",
        "These can take any numeric value within a range.\n",
        "They are measurable.\n",
        "Example: Height, weight, temperature, income, age.\n",
        "\n",
        "2. Categorical Variables\n",
        "These represent categories or groups.\n",
        "They are not numeric (or used as labels even if numeric).\n",
        "Example: Gender (male/female), color (red/blue), city name, blood type.\n",
        "\n",
        "In short:\n",
        "Continuous = numbers you can measure.\n",
        "Categorical = labels or groups.\"\"\""
      ],
      "metadata": {
        "id": "sCCIepA0OVu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6 How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\"\"\"Ans\n",
        "Categorical variables are variables that are non-numeric. It is converted into numeric data by the process of Data encoding. It is required as machine uses numeric data for process.\n",
        "It's common teechniques are:-\n",
        "\n",
        "Nominal:- Categorical into numeric data. There is no order involved. Although curse of dimensionality may cause problems\n",
        "Label and ordinal encoding:- it assigns a numeric number for each category.Ordinal encoding as a assigned order for each category.\n",
        "Target guided nominal encoding:- Target varibles are replaced by the mean/median. Useful when there are n unique variables present.\"\"\""
      ],
      "metadata": {
        "id": "40CpIpLtOlbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7 What do you mean by training and testing a dataset?\n",
        "\"\"\"Ans\n",
        " Data is divided into two parts for a model of ML.One for training and the other for testing.It is usually in the 80:20 ratio where training data is mostly used.\n",
        "The training dataset is used by the model to gain insights,patterns etc to gain useful info from data. The model is now trained.\n",
        "This model is then tested with unseen data also called as testing dataset.It helps detemine that our ML models are accurate or not.\"\"\""
      ],
      "metadata": {
        "id": "aBMlC3MzO6Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8 What is sklearn.preprocessing?\n",
        "\"\"\"Ans\n",
        "sklearn.preprocessing is a module in scikit-learn that provides tools for preprocessing data before using it in a machine learning model. It provides function such as:-\n",
        "MinxMAx Scaling\n",
        "Standardization\n",
        "Normaliztion etc\"\"\""
      ],
      "metadata": {
        "id": "LyrDsiCzPcWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9 What is a Test set?\n",
        "\"\"\"Ans\n",
        "A test set is a unseen part of the dataset.The dataset used to train and test model is divided in ratio 80:20. Most data is used to train the model and also validate the model.\n",
        "But a part of data is kept unseen from model. This is the test set which is used at the end to check if the model produces accurate results with unseen data.\"\"\""
      ],
      "metadata": {
        "id": "fWTvCn7bPpzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q10 How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\"\"\"Ans\n",
        "We split data into the ratio 80:20. The 80 percent of data is used for training and validation. Training helps to discover patterns in data. Validation is done to check accuracy of data on known values.\n",
        " The training dataset is in the 60:20 ratio where 60 is data training and 20 is for validation. The rest 20 percent of data is unseen for model and is used for testing. It helps check if the model is accurate on unseen data.\n",
        "\n",
        "Approach of a machine learning problem:-\n",
        "Understand the Problem\n",
        "Collect and Prepare Data\n",
        "Exploratory Data Analysis (EDA)\n",
        "Feature Engineering\n",
        "Split the Data\n",
        "Select and Train a Model\n",
        "Evaluate the Model\n",
        "Tune Hyperparameters\n",
        "Test and Deploy\n",
        "Monitor and Update.\"\"\""
      ],
      "metadata": {
        "id": "9NAC5TYoP8PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q11 Why do we have to perform EDA before fitting a model to the data?\n",
        "\"\"\"Ans\n",
        "EDA is also termed as Exploratory Data Analysis. It uses models/plots to determine the patterns , relationships ,insights etc in variables.\n",
        " It is also used to determine whether the data has class imbalance or not. Hence why is important to perfrom EDA before fitting of Data.\"\"\""
      ],
      "metadata": {
        "id": "0_rfWGstQhD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q12 What is correlation?\n",
        "\"\"\"Ans\n",
        "Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It ranges from -1 to +1:\n",
        " +1 means a perfect positive correlation (as one increases, the other increases).\n",
        " 0 means no correlation (no relationship).\n",
        " -1 means a perfect negative correlation (as one increases, the other decreases).\"\"\""
      ],
      "metadata": {
        "id": "BAfZplJVQ0F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q13 What does negative correlation mean?\n",
        "\"\"\"Ans\n",
        "A negative correlation occurs when one variable increases, the other decreases. Let's say your game time increases as your study time decreases. It has a value of -1.\"\"\""
      ],
      "metadata": {
        "id": "5W4icqnORD8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q14 How can you find correlation between variables in Python?\n",
        "\"\"\"Ans\n",
        "You can find correlation between variables in Python using Pandas or Seaborn. It's value ranges between -1 and +1. Here's how:\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "df = pd.read_csv('your_data.csv')#GIve any file or data\n",
        "\n",
        "# Get correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "id": "5zPvmMofRV-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q15 What is causation? Explain difference between correlation and causation with an example?\n",
        "\"\"\"Ans\n",
        "Causation means that one variable directly affects or causes a change in another. It shows a cause-and-effect relationship.\n",
        " On the other hand, correlation simply shows that two variables have a relationship or pattern, but it does not mean that one causes the other. For example,\n",
        "  there may be a correlation between ice cream sales and drowning cases because both increase in summer, but eating ice cream does not cause drowning.\n",
        "  This is just a correlation, not causation. In contrast, smoking causing lung disease is an example of causation because research shows that smoking directly leads to health problems.\n",
        " So, while correlation shows a connection, causation proves that one thing leads to another.\"\"\""
      ],
      "metadata": {
        "id": "b3Yt9lxMSsnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q16 What is an Optimizer? What are different types of optimizers? Explain each with an example?\n",
        "\"\"\"Ans\n",
        "An optimizer is an algorithm that helps a machine learning model learn by adjusting its internal parameters (like weights) to minimize the error or loss during training.\n",
        "It uses the gradients of the loss function to update the weights in the right direction, improving the model's performance over time.\n",
        "\n",
        "Gradient Descent (GD): Uses the full dataset to compute gradients and update weights. It's stable but slow and not ideal for large datasets.\n",
        "\n",
        "Stochastic Gradient Descent (SGD): Updates weights after each data point. It's faster and works well with large data, but updates can be noisy.\n",
        "\n",
        "Mini-Batch Gradient Descent: A balanced version that uses small batches of data. It combines the speed of SGD and the stability of GD.\n",
        "\n",
        "Momentum: Enhances SGD by adding a fraction of the previous update to the current one, helping the model move faster through flat regions.\n",
        "\n",
        "RMSprop: Adjusts learning rates based on recent gradients, making it effective for problems like time-series and RNNs.\n",
        "\n",
        "Adam (Adaptive Moment Estimation): Combines both momentum and RMSprop. It's widely used because it adapts learning rates and converges faster.\"\"\""
      ],
      "metadata": {
        "id": "GTkpGPlPTKZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q17 What is sklearn.linear_model ?\n",
        "\"\"\"Ans\n",
        "sklearn.linear_model is a module in the scikit-learn library that provides linear models for machine learning tasks like regression and classification.\n",
        "It includes popular algorithms such as:\n",
        "LinearRegression – for predicting continuous values using simple or multiple linear regression.\n",
        "\n",
        "LogisticRegression – for binary or multi-class classification problems.\n",
        "\n",
        "Ridge, Lasso, and ElasticNet – for regularized linear regression that helps prevent overfitting.\n",
        "\n",
        "SGDClassifier and SGDRegressor – use stochastic gradient descent for large-scale problems.\"\"\""
      ],
      "metadata": {
        "id": "_q5fUwjwTgGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q18 What does model.fit() do? What arguments must be given?\n",
        "\"\"\"Ans\n",
        "The model.fit() function is used to train a machine learning model using the provided training data. It adjusts the internal parameters of the model so that it can learn the relationship between the input features and the output labels.\n",
        "Key points:\n",
        "\n",
        "It is the core method used to fit or train a model on data.\n",
        "\n",
        "Takes two main arguments:\n",
        "\n",
        "X: Feature data (input)\n",
        "\n",
        "y: Target labels (output)\n",
        "\n",
        "After fitting, the model is ready to make predictions using model.predict().\"\"\""
      ],
      "metadata": {
        "id": "qaj3Qh0lTxg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q19 What does model.predict() do? What arguments must be given?\n",
        "\"\"\"Ans\n",
        "The model.predict() function is used to generate predictions from a trained machine learning model. After the model has been trained using model.fit(),\n",
        "this function estimates the output for new input data based on the patterns it has learned.\n",
        "It is used for prediction or inference after training is complete.\n",
        "\n",
        "Takes one required argument:\n",
        "\n",
        "X: Input features (data to be predicted, same format as training features).\n",
        "\n",
        "Returns predicted values (e.g., class labels for classification or numerical values for regression).\"\"\""
      ],
      "metadata": {
        "id": "aBlrMnfxUTjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q20 What are continuous and categorical variables?\n",
        "\"\"\"Ans\n",
        "Continuous variables and categorical variables are two types of data:\n",
        "\n",
        "1. Continuous Variables\n",
        "These can take any numeric value within a range.\n",
        "They are measurable.\n",
        "Example: Height, weight, temperature, income, age.\n",
        "\n",
        "2. Categorical Variables\n",
        "These represent categories or groups.\n",
        "They are not numeric (or used as labels even if numeric).\n",
        "Example: Gender (male/female), color (red/blue), city name, blood type.\n",
        "\n",
        "In short:\n",
        "Continuous = numbers you can measure.\n",
        "Categorical = labels or groups.\"\"\"\n"
      ],
      "metadata": {
        "id": "KFixGm7ZUkXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q21 What is feature scaling? How does it help in Machine Learning?\n",
        "\"\"\"Ans\n",
        "Feature scaling is the process of adjusting the range of input features so that they are on a similar scale.\n",
        "This is important in machine learning because many algorithms, especially those that rely on distance or gradient calculations like k-NN, SVM, and gradient descent-based models, perform better when all features contribute equally.\n",
        "If one feature has a much larger range than others, it can dominate the learning process and lead to inaccurate results.\n",
        "Feature scaling helps by normalizing or standardizing the values—either by rescaling them between 0 and 1 (normalization) or by adjusting them to have a mean of 0 and a standard deviation of 1 (standardization).\n",
        "This leads to faster training, more stable models, and often better performance.\"\"\""
      ],
      "metadata": {
        "id": "4yDHaDTbUzuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q22 How do we perform scaling in Python?\n",
        "\"\"\"Ans\n",
        "In Python, scaling is typically performed using the sklearn.preprocessing module from the scikit-learn library. The most common techniques are StandardScaler for standardization and MinMaxScaler for normalization.\n",
        "You first import the scaler, fit it to your training data using the fit() method, and then apply the transformation using the transform() or fit_transform() method.\n",
        "This process adjusts the feature values so they are on the same scale, which is especially important before training models that are sensitive to feature magnitude.\"\"\""
      ],
      "metadata": {
        "id": "P61NLVGPVYGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q23 What is sklearn.preprocessing?\n",
        "\"\"\"Ans\n",
        "sklearn.preprocessing is a module in scikit-learn that provides tools for preprocessing data before using it in a machine learning model. It provides function such as:-\n",
        "MinxMAx Scaling\n",
        "Standardization\n",
        "Normaliztion etc\"\"\""
      ],
      "metadata": {
        "id": "Z5vz7133Vve8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q24 How do we split data for model fitting (training and testing) in Python?\n",
        "\"\"\"Ans\n",
        "We split data into the ratio 80:20. The 80 percent of data is used for training and validation.\n",
        "Training helps to discover patterns in data. Validation is done to check accuracy of data on known values. The training dataset is in the 60:20 raion where 60 is data training and 20 is for validation.\n",
        "The rest 20 percent of data is unseen for model and is used for testing. It helps check if the model is accurate on unseen data.\"\"\""
      ],
      "metadata": {
        "id": "pVCjgkGnWKOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q25 Explain data encoding?\n",
        "\"\"\"Ans\n",
        "Data encoding is the process of converting categorical or textual data into a numerical format that can be used by machine learning algorithms.\n",
        "Since most models work only with numbers, encoding helps transform labels or categories into a suitable numerical form.\n",
        "There are different types of encoding methods, such as label encoding, where each category is assigned a unique integer, and one-hot encoding, where each category is represented by a binary vector.\n",
        "Proper encoding ensures that the model correctly interprets the relationships between categories and avoids giving unintended priority or weight to specific values.\"\"\""
      ],
      "metadata": {
        "id": "XjhT2nQHWaQ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}